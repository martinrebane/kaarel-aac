{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5bc4096ab0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SequenceTaggingDataset(data.Dataset):\n",
    "    \"\"\"Defines a dataset for sequence tagging. Examples in this dataset\n",
    "    contain paired lists -- paired list of words and tags.\n",
    "    For example, in the case of part-of-speech tagging, an example is of the\n",
    "    form\n",
    "    [I, love, PyTorch, .] paired with [PRON, VERB, PROPN, PUNCT]\n",
    "    See torchtext/test/sequence_tagging.py on how to use this class.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(example):\n",
    "        for attr in dir(example):\n",
    "            if not callable(getattr(example, attr)) and \\\n",
    "                    not attr.startswith(\"__\"):\n",
    "                return len(getattr(example, attr))\n",
    "        return 0\n",
    "\n",
    "    def __init__(self, path, fields, separator=\"\\t\", **kwargs):\n",
    "        examples = []\n",
    "        columns = []\n",
    "        print('path:', path)\n",
    "        with open(path) as input_file:\n",
    "            for line in input_file:\n",
    "                line = line.strip()\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "                elif line == \"\":\n",
    "                    if columns:\n",
    "                        examples.append(data.Example.fromlist(columns, fields))\n",
    "                    columns = []\n",
    "                else:\n",
    "                    for i, column in enumerate(line.split(separator)):\n",
    "                        if len(columns) < i + 1:\n",
    "                            columns.append([])\n",
    "                        columns[i].append(column)\n",
    "\n",
    "            if columns:\n",
    "                examples.append(data.Example.fromlist(columns, fields))\n",
    "        super(SequenceTaggingDataset, self).__init__(examples, fields,\n",
    "                                                     **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "WORD = data.Field(init_token=\"<bos>\", eos_token=\"<eos>\", include_lengths=True, lower=True, batch_first=True)\n",
    "UD_TAG = data.Field(init_token=\"<bos>\", eos_token=\"<eos>\", pad_token=None, unk_token=None, batch_first=True)\n",
    "\n",
    "CHAR_NESTING = data.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "CHAR = data.NestedField(CHAR_NESTING, init_token=\"<bos>\", eos_token=\"<eos>\", include_lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UDPOSMorph(SequenceTaggingDataset):\n",
    "\n",
    "    # Universal Dependencies English Web Treebank.\n",
    "    # Download original at http://universaldependencies.org/\n",
    "    # License: http://creativecommons.org/licenses/by-sa/4.0/\n",
    "    # urls = ['https://bitbucket.org/sivareddyg/public/downloads/en-ud-v2.zip']\n",
    "    # dirname = 'en-ud-v2'\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, fields, root='data', train=None, validation=None,\n",
    "               test=None, **kwargs):\n",
    "        cls.name = \"\"\n",
    "        cls.dirname = \"\"\n",
    "\n",
    "        return super(UDPOSMorph, cls).splits(\n",
    "            fields=fields, root=root, train=train, validation=validation,\n",
    "            test=test, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: data/et-ud-train.conllu\n",
      "path: data/et-ud-dev.conllu\n",
      "path: data/et-ud-test.conllu\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = UDPOSMorph.splits(\n",
    "    fields=((None, None), (('word', 'char'), (WORD, CHAR)), (None, None), ('udtag', UD_TAG)),\n",
    "    root='data', train='et-ud-train.conllu', validation='et-ud-dev.conllu', test='et-ud-test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: data/et-ud-train.conllu\n",
      "path: data/et-ud-dev.conllu\n",
      "path: data/et-ud-test.conllu\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = UDPOSMorph.splits(\n",
    "    fields=((None, None), ('char', CHAR), (None, None), ('udtag', UD_TAG)),\n",
    "    root='data', train='et-ud-train.conllu', validation='et-ud-dev.conllu', test='et-ud-test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'char': [['Ö', 'ö'], ['o', 'l', 'i'], ['t', 'ä', 'i', 'e', 's', 't', 'i'], ['t', 'u', 'u', 'l', 'e', 't', 'u'], ['.']], 'udtag': ['NOUN', 'VERB', 'ADV', 'ADJ', 'PUNCT']}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0].__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "\n",
    "train_data, valid_data, test_data = datasets.UDPOS.splits(\n",
    "    fields=((('word', 'char'), (WORD, CHAR)), ('udtag', UD_TAG)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 14510\n",
      "Number of validation examples: 1793\n",
      "Number of testing examples: 1806\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext import vocab\n",
    "import os\n",
    "\n",
    "glove_embeds = vocab.GloVe(name=\"6B\", dim=300, cache=\"/Users/kairit/Projects/textcnn/notebooks/.vector_cache\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/329988 [00:00<?, ?it/s]Skipping token 329987 with 1-dimensional vector ['300']; likely a header\n",
      "100%|██████████| 329988/329988 [02:15<00:00, 2437.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import FastText\n",
    "fasttext_embeds = FastText(language='et')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WORD.build_vocab(train_data)\n",
    "# WORD.vocab.extend(fasttext_embeds)\n",
    "# WORD.vocab.load_vectors(fasttext_embeds)\n",
    "\n",
    "CHAR.build_vocab(train_data)\n",
    "UD_TAG.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([343685, 300])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD.vocab.vectors.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in CHAR vocabulary: 120\n",
      "Unique tokens in UD_TAG vocabulary: 17\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Unique tokens in WORD vocabulary: {len(WORD.vocab)}\")\n",
    "print(f\"Unique tokens in CHAR vocabulary: {len(CHAR.vocab)}\")\n",
    "print(f\"Unique tokens in UD_TAG vocabulary: {len(UD_TAG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        super(CharEmbeddings, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embed_dropout = nn.Dropout(p=0.5)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def forward(self, chars, lengths):\n",
    "        chars_size = chars.size()\n",
    "        \n",
    "        # Aggregate sequence length and batch dimensions\n",
    "        chars = chars.reshape(-1, chars_size[-1])\n",
    "        \n",
    "        # Embed characters\n",
    "        embeds = self.char_embeddings(chars)\n",
    "        embeds = self.embed_dropout(embeds)\n",
    "        # Move batch to second dimension\n",
    "        \n",
    "        # Concat the batch and sentence dimensions of word lengths\n",
    "        lengths = lengths.reshape(-1)\n",
    "        \n",
    "        # Sort lengths in descending order\n",
    "        lengths_sort, idx_sort = torch.sort(lengths, dim=0, descending=True)\n",
    "        # Replace 0 entries with 1s\n",
    "        lengths_sort[lengths_sort==0] = 1\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "    \n",
    "        # Sort embeddings\n",
    "        embeds_sort = embeds.index_select(0, idx_sort)\n",
    "        # Pack sorted embeddings\n",
    "        embeds_pack = pack_padded_sequence(embeds_sort, lengths_sort, batch_first=True)\n",
    "        \n",
    "        # Send the pack through LSTM\n",
    "        _, hidden = self.lstm(embeds_pack)\n",
    "        \n",
    "        # Concatenate states to get word embeddings\n",
    "        word_embeds = torch.cat(hidden, dim=2)\n",
    "        # Move directionality dimension to second position\n",
    "        word_embeds = word_embeds.permute(1, 0, 2)\n",
    "        # Reshape to (batch x sequence) x dimension\n",
    "        word_embeds = word_embeds.reshape(-1, self.hidden_dim * 4)\n",
    "        \n",
    "        # Restore the original index ordering\n",
    "        word_embeds = word_embeds.index_select(0, idx_unsort)\n",
    "        \n",
    "        # Reshape back to original shape\n",
    "        word_embeds = word_embeds.reshape(chars_size[0], chars_size[1], -1)\n",
    "        # Permute axes to sequence x batch x dimension\n",
    "        # word_embeds = word_embeds.permute(1, 0, 2)\n",
    "        return word_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, word_embedding_dim, embedding_dim, hidden_dim, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.special_embeddings = nn.Embedding(5, word_embedding_dim, padding_idx=-1)\n",
    "        # self.word_embeddings.weight.requires_grad=False\n",
    "        \n",
    "        self.embed2input = nn.Linear(word_embedding_dim, word_embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(2*hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, word_embeds, words, lengths, char_embeds=None):\n",
    "        # embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "        mask = ~((words == WORD.vocab.stoi['<bos>']) | (words == WORD.vocab.stoi['<eos>']) \n",
    "                | (words == WORD.vocab.stoi['<unk>']) | (words == WORD.vocab.stoi['<pad>']))\n",
    "        words[mask] = 4\n",
    "        \n",
    "        special_embeds = self.special_embeddings(words)\n",
    "        # print(special_embeds)\n",
    "        \n",
    "        word_embeds = word_embeds + special_embeds\n",
    "        \n",
    "        embeds = self.embed2input(word_embeds)\n",
    "        if char_embeds is not None:\n",
    "            embeds = torch.cat([embeds, char_embeds], dim=2)\n",
    "        \n",
    "        lengths = lengths.reshape(-1)\n",
    "        embeds_pack = pack_padded_sequence(embeds, lengths, batch_first=True)\n",
    "        lstm_pack_out, _ = model.lstm(embeds_pack)\n",
    "\n",
    "        lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_pack_out, batch_first=True)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "WORD_EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 300\n",
    "CHAR_EMBEDDING_DIM = 100\n",
    "CHAR_HIDDEN_DIM = 100\n",
    "EMBEDDING_DIM = WORD_EMBEDDING_DIM + 4 * CHAR_EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_within_batch=True,\n",
    "    repeat=False,\n",
    "    device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "char_model = CharEmbeddings(CHAR_EMBEDDING_DIM, CHAR_HIDDEN_DIM, len(CHAR.vocab))\n",
    "\n",
    "model = LSTMTagger(WORD_EMBEDDING_DIM, EMBEDDING_DIM, HIDDEN_DIM, len(UD_TAG.vocab))\n",
    "# pretrained_embeddings = WORD.vocab.vectors\n",
    "# model.word_embeddings.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=WORD.vocab.stoi['<pad>'])\n",
    "\n",
    "params = list(char_model.parameters()) + list(model.parameters())\n",
    "# optimizer = optim.SGD(params, lr=0.1)\n",
    "optimizer = optim.Adam(params)\n",
    "\n",
    "char_model = char_model.to(device)\n",
    "model = model.to(device)\n",
    "loss_function = loss_function.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, char_model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    char_model.train()\n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        words, lengths = batch.word\n",
    "        chars, _, char_lengths = batch.char\n",
    "        char_embeddings = char_model(chars, char_lengths)\n",
    "        # print('char embeddings:', char_embeddings.size())\n",
    "        \n",
    "        word_embeddings = F.embedding(words, WORD.vocab.vectors)\n",
    "        predictions = model(word_embeddings, words, lengths, char_embeddings)\n",
    "        predictions = predictions.reshape(-1, predictions.size()[-1])\n",
    "        labels = batch.udtag.reshape(-1)\n",
    "        words = words.reshape(-1)\n",
    "\n",
    "        # Step 3. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = sequence_accuracy(words, predictions, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'| Batch: {i:02} | Batch Loss: {loss:.3f} | Batch Acc: {acc*100:.2f}%')\n",
    "            # print(model.bos_embed)\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, char_model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_oov_acc = 0\n",
    "    oov_batches = 0\n",
    "    \n",
    "    model.eval()\n",
    "    char_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            words, lengths = batch.word\n",
    "            chars, _, char_lengths = batch.char\n",
    "            char_embeddings = char_model(chars, char_lengths)\n",
    "            \n",
    "            word_embeddings = F.embedding(words, WORD.vocab.vectors)\n",
    "            predictions = model(word_embeddings, words, lengths, char_embeddings)\n",
    "            predictions = predictions.reshape(-1, predictions.size()[-1])\n",
    "            labels = batch.udtag.reshape(-1)\n",
    "            words = words.reshape(-1)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = sequence_accuracy(words, predictions, labels)\n",
    "    \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            oov_acc = oov_accuracy(words, predictions, labels)\n",
    "            if oov_acc is not None:\n",
    "                epoch_oov_acc += oov_acc\n",
    "                oov_batches += 1\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_oov_acc / oov_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def oov_accuracy(words, scores, targets):\n",
    "    _, predictions = torch.max(scores, 1)\n",
    "\n",
    "    data = [[pred, lab] for (word, pred, lab) in zip(words, predictions, targets)]\n",
    "\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        data = np.array(data)\n",
    "        return np.mean(data[:,0] == data[:,1])\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Batch: 00 | Batch Loss: 2.831 | Batch Acc: 7.08%\n",
      "| Batch: 10 | Batch Loss: 1.584 | Batch Acc: 40.18%\n",
      "| Batch: 20 | Batch Loss: 1.143 | Batch Acc: 56.36%\n",
      "| Batch: 30 | Batch Loss: 1.185 | Batch Acc: 60.40%\n",
      "| Batch: 40 | Batch Loss: 0.936 | Batch Acc: 71.65%\n",
      "| Batch: 50 | Batch Loss: 0.683 | Batch Acc: 76.22%\n",
      "| Batch: 60 | Batch Loss: 0.550 | Batch Acc: 80.56%\n",
      "| Batch: 70 | Batch Loss: 0.295 | Batch Acc: 89.58%\n",
      "| Batch: 80 | Batch Loss: 0.428 | Batch Acc: 86.05%\n",
      "| Batch: 90 | Batch Loss: 0.395 | Batch Acc: 85.34%\n",
      "| Batch: 100 | Batch Loss: 0.444 | Batch Acc: 86.19%\n",
      "| Batch: 110 | Batch Loss: 0.277 | Batch Acc: 90.23%\n",
      "| Batch: 120 | Batch Loss: 0.369 | Batch Acc: 89.09%\n",
      "| Batch: 130 | Batch Loss: 0.276 | Batch Acc: 91.29%\n",
      "| Batch: 140 | Batch Loss: 0.366 | Batch Acc: 89.36%\n",
      "| Batch: 150 | Batch Loss: 0.225 | Batch Acc: 91.52%\n",
      "| Batch: 160 | Batch Loss: 0.280 | Batch Acc: 87.97%\n",
      "| Batch: 170 | Batch Loss: 0.283 | Batch Acc: 89.79%\n",
      "| Batch: 180 | Batch Loss: 0.245 | Batch Acc: 91.02%\n",
      "| Batch: 190 | Batch Loss: 0.355 | Batch Acc: 90.95%\n",
      "| Batch: 200 | Batch Loss: 0.258 | Batch Acc: 90.10%\n",
      "| Batch: 210 | Batch Loss: 0.363 | Batch Acc: 93.42%\n",
      "| Batch: 220 | Batch Loss: 0.277 | Batch Acc: 92.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kairit/.pyenv/versions/my-virtual-env-3.6.2/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n",
      "/Users/kairit/.pyenv/versions/my-virtual-env-3.6.2/lib/python3.6/site-packages/torchtext/data/field.py:321: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train), lengths\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.587 | Train Acc: 81.11% | Val. Loss: 0.525 | Val. Acc: 88.64% | Val. OOV Acc: 82.72% |\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc  = train(model, char_model, train_iterator, optimizer, loss_function)\n",
    "    valid_loss, valid_acc, oov_acc = evaluate(model, char_model, valid_iterator, loss_function)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% | Val. OOV Acc: {oov_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13253304263462826 0.9542531751248301 0.872427053548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kairit/.pyenv/versions/my-virtual-env-3.6.2/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n",
      "/Users/kairit/.pyenv/versions/my-virtual-env-3.6.2/lib/python3.6/site-packages/torchtext/data/field.py:321: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train), lengths\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1756999532226473 0.9302465580403805 0.845655787727\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc, train_oov_acc = evaluate(model, char_model, train_iterator, loss_function)\n",
    "print(train_loss, train_acc, train_oov_acc)\n",
    "valid_loss, valid_acc, oov_acc = evaluate(model, char_model, valid_iterator, loss_function)\n",
    "print(valid_loss, valid_acc, oov_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 64 from UDPOS]\n",
      "\t[.udtag]:[torch.LongTensor of size 64x15]\n",
      "\t[.word]:('[torch.LongTensor of size 64x15]', '[torch.LongTensor of size 64]')\n",
      "\t[.char]:('[torch.LongTensor of size 64x15x17]', '[torch.LongTensor of size 64]', '[torch.LongTensor of size 64x15]')\n"
     ]
    }
   ],
   "source": [
    "item = next(train_iterator.__iter__())\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: torch.Size([64, 15]) tensor([[     2,     14,     57,    687,     15,     14,    254,    110,\n",
      "              7,    686,      9,   8387,    784,      5,      3],\n",
      "        [     2,     51,     26,    131,    988,   1918,   3702,      4,\n",
      "           4058,   2026,     23,   1664,   1062,      5,      3],\n",
      "        [     2,     11,   2848,     16,     67,     66,     16,     22,\n",
      "           1403,      8,     22,    539,   9354,      5,      3],\n",
      "        [     2,      4,    193,    136,      6,      4,    682,    930,\n",
      "           1159,      4,     90,   1108,    439,      5,      3],\n",
      "        [     2,  10226,     21,      4,    528,   1025,      6,  14137,\n",
      "              8,      4,     81,   3095,  12274,      5,      3],\n",
      "        [     2,    311,    395,     13,     45,    292,      6,     40,\n",
      "             14,     26,    237,   2520,    876,      5,      3],\n",
      "        [     2,    131,    501,   6860,     26,    452,      7,    391,\n",
      "            302,   4071,     17,    494,   1611,      5,      3],\n",
      "        [     2,     16,     36,   5311,   1772,    585,      8,     90,\n",
      "            207,   3200,     17,   1644,   2452,      5,      3],\n",
      "        [     2,    129,    178,     17,    383,      4,     64,      7,\n",
      "           1420,      8,    384,     23,     49,      5,      3],\n",
      "        [     2,    141,     43,    508,     89,     12,     66,     37,\n",
      "            921,      7,     17,     84,    531,      5,      3],\n",
      "        [     2,     41,    121,    608,     19,    780,   1423,     16,\n",
      "             31,      9,  15909,   5801,      5,     20,      3],\n",
      "        [     2,     13,     16,   1235,     17,    977,      7,   1775,\n",
      "             67,   1022,    195,    293,   5392,     39,      3],\n",
      "        [     2,     15,    183,    582,     73,   2338,     37,   4367,\n",
      "            949,     47,  13107,     32,   8897,      5,      3],\n",
      "        [     2,     16,     31,   1008,     32,   1347,      8,   4209,\n",
      "              9,   1524,    267,      9,    376,      5,      3],\n",
      "        [     2,     52,     37,     31,    125,      7,     19,      7,\n",
      "            357,    241,     17,    165,    120,      5,      3],\n",
      "        [     2,     11,     57,      9,    269,     23,      4,   3340,\n",
      "             12,     32,   1477,    540,   2150,      5,      3],\n",
      "        [     2,    815,   1243,   1381,      6,   4594,     54,    270,\n",
      "              7,      4,    671,   3769,    235,      5,      3],\n",
      "        [     2,     37,    338,     50,    320,     14,    346,     49,\n",
      "             14,   1167,     12,   2134,   8645,     39,      3],\n",
      "        [     2,     69,    141,     49,     75,      8,     11,     47,\n",
      "            345,     14,      9,     90,   1968,      5,      3],\n",
      "        [     2,    399,     22,     52,    466,     15,     11,     47,\n",
      "             28,    430,      7,     89,    102,   1185,      3],\n",
      "        [     2,     11,     48,    394,    244,   1374,    121,     98,\n",
      "             67,     17,     43,    193,    208,      5,      3],\n",
      "        [     2,   2874,     44,      4,    473,    175,   1545,  14380,\n",
      "             31,   2344,     13,    125,     21,      5,      3],\n",
      "        [     2,   2631,    248,     18,    135,     10,   7122,     73,\n",
      "           2947,     17,    109,     12,    536,      5,      3],\n",
      "        [     2,     14,     26,   5629,     12,     43,    108,      8,\n",
      "             16,    876,     12,     32,   4669,      5,      3],\n",
      "        [     2,   1154,      4,   6055,   3680,    259,     29,     14,\n",
      "            568,   6637,   8275,      8,  13794,      5,      3],\n",
      "        [     2,     35,     28,     89,     51,     41,     14,    659,\n",
      "              7,   1348,    191,      4,    238,      5,      3],\n",
      "        [     2,   2928,     13,     12,   2655,     21,    374,      6,\n",
      "             40,     13,    498,     46,    283,      5,      3],\n",
      "        [     2,     94,     35,     25,   1605,     15,     24,    207,\n",
      "           1125,     13,    630,    610,    483,      5,      3],\n",
      "        [     2,     37,     31,    159,  14970,    656,      8,     28,\n",
      "            293,    131,    467,    247,   4638,      5,      3],\n",
      "        [     2,     24,     36,     68,     14,     21,      4,   8246,\n",
      "            520,    365,    125,    123,    473,      5,      3],\n",
      "        [     2,     11,    122,    180,     24,    454,     10,    246,\n",
      "            859,     21,     54,      4,     64,      5,      3],\n",
      "        [     2,     11,     19,     24,   2022,    566,      6,   1280,\n",
      "             44,      4,   2022,   7050,   3889,      5,      3],\n",
      "        [     2,     11,    122,     28,   1036,    706,     29,      7,\n",
      "             61,     14,     26,    241,     17,      5,      3],\n",
      "        [     2,     96,    303,     47,    304,     25,   1387,     17,\n",
      "              9,    207,     10,     24,   1119,     39,      3],\n",
      "        [     2,    708,     13,      4,    155,    466,      8,   2694,\n",
      "           8031,     15,     11,     19,    202,      5,      3],\n",
      "        [     2,    409,    588,      4,   8426,     10,    446,      6,\n",
      "           9780,      4,    113,     22,    890,      5,      3],\n",
      "        [     2,      4,    625,     22,   6091,      8,      6,     12,\n",
      "             72,   1682,      6,     81,   2694,      5,      3],\n",
      "        [     2,   9184,  10025,      6,  12750,  11632,     10,  14265,\n",
      "              6,   3618,    188,  13213,   1285,      5,      3],\n",
      "        [     2,     94,    166,     49,      9,    145,     41,     14,\n",
      "             48,     63,      7,    522,      5,      3,      1],\n",
      "        [     2,    275,     70,    127,      6,    109,    367,    305,\n",
      "           5620,     29,      9,   1434,      5,      3,      1],\n",
      "        [     2,    103,   1154,     15,   2773,   2043,     13,   1929,\n",
      "             14,     12,    213,   3064,      5,      3,      1],\n",
      "        [     2,     11,     35,     28,    137,      4,   1242,    657,\n",
      "            163,    191,      7,    734,      5,      3,      1],\n",
      "        [     2,   2374,     46,     50,   5405,     18,   1855,     21,\n",
      "            774,   1514,    462,   9256,   3719,      3,      1],\n",
      "        [     2,     37,  16294,      4,  12066,   1386,      7,  11477,\n",
      "              4,   1183,     31,   1635,      5,      3,      1],\n",
      "        [     2,     14,     47,   5204,    439,     21,      9,    113,\n",
      "           1228,     23,    786,    457,      5,      3,      1],\n",
      "        [     2,     95,   9270,    520,     18,    772,     10,   6564,\n",
      "           3048,   1485,     12,    179,      5,      3,      1],\n",
      "        [     2,     21,      4,   3013,    672,      6,     43,   5350,\n",
      "             36,     25,    378,   3837,      5,      3,      1],\n",
      "        [     2,     37,    153,    146,    813,     43,   7026,    123,\n",
      "           2559,     17,     43,    201,     53,      3,      1],\n",
      "        [     2,      4,   4271,  14296,     13,  10281,     12,    514,\n",
      "            446,    597,     10,  13569,      5,      3,      1],\n",
      "        [     2,     40,    162,    103,     16,    143,      4,    155,\n",
      "           3493,    246,      7,     35,      5,      3,      1],\n",
      "        [     2,     35,     14,     19,      7,     89,    191,      9,\n",
      "           1321,   3553,    162,    259,     39,      3,      1],\n",
      "        [     2,      8,     11,   1974,     92,     31,     21,      4,\n",
      "            283,     23,  15320,    174,    358,      3,      1],\n",
      "        [     2,    662,     61,      6,     22,     28,     12,      4,\n",
      "            863,    296,     29,    111,      5,      3,      1],\n",
      "        [     2,   8005,     13,   1495,     59,    175,  16450,    563,\n",
      "           6279,  14606,      7,   7923,      5,      3,      1],\n",
      "        [     2,    199,      5,   4937,  12235,      6,   1683,      6,\n",
      "            682,   3777,      6,   8699,      5,      3,      1],\n",
      "        [     2,     11,    218,      4,    566,      8,    120,     16,\n",
      "            193,      7,      4,   4129,      5,      3,      1],\n",
      "        [     2,     11,    171,    152,   1229,      9,   1310,     15,\n",
      "             86,     45,   1802,     49,      5,      3,      1],\n",
      "        [     2,    339,     27,   1200,     13,      9,   2504,      6,\n",
      "              8,     24,    476,     54,      5,      3,      1],\n",
      "        [     2,      4,   1666,    725,     44,   1053,   5028,      7,\n",
      "              9,   4291,     10,   7413,      5,      3,      1],\n",
      "        [     2,     11,     36,    166,     74,      8,    145,      8,\n",
      "            117,     61,     27,    164,      5,      3,      1],\n",
      "        [     2,     37,     57,      9,  15001,     15,     22,    158,\n",
      "            102,     12,      4,   8890,      5,      3,      1],\n",
      "        [     2,   1241,      6,     11,     36,   1670,     14,     36,\n",
      "            635,      4,    138,   2602,      5,      3,      1],\n",
      "        [     2,     78,      6,     51,     97,     28,     25,   8002,\n",
      "             54,    126,      4,   1597,      5,      3,      1],\n",
      "        [     2,     11,     48,    244,     16,    939,    182,      8,\n",
      "             79,      9,   7694,    387,      5,      3,      1]])\n",
      "mask: torch.Size([64, 15]) tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          0],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1],\n",
      "        [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "          1]], dtype=torch.uint8)\n",
      "word embeds: torch.Size([64, 15, 300])\n",
      "bos embeds: torch.Size([64, 300]) tensor([[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]])\n",
      "bos embeds: torch.Size([64, 300]) tensor([[ 1.1000,  1.1000,  1.1000,  ...,  1.1000,  1.1000,  1.1000],\n",
      "        [ 1.1000,  1.1000,  1.1000,  ...,  1.1000,  1.1000,  1.1000],\n",
      "        [ 1.1000,  1.1000,  1.1000,  ...,  1.1000,  1.1000,  1.1000],\n",
      "        ...,\n",
      "        [ 1.1000,  1.1000,  1.1000,  ...,  1.1000,  1.1000,  1.1000],\n",
      "        [ 1.1000,  1.1000,  1.1000,  ...,  1.1000,  1.1000,  1.1000],\n",
      "        [ 1.1000,  1.1000,  1.1000,  ...,  1.1000,  1.1000,  1.1000]])\n",
      "eos embeds: torch.Size([64, 300]) tensor([[ 2.2000,  2.2000,  2.2000,  ...,  2.2000,  2.2000,  2.2000],\n",
      "        [ 2.2000,  2.2000,  2.2000,  ...,  2.2000,  2.2000,  2.2000],\n",
      "        [ 2.2000,  2.2000,  2.2000,  ...,  2.2000,  2.2000,  2.2000],\n",
      "        ...,\n",
      "        [ 2.2000,  2.2000,  2.2000,  ...,  2.2000,  2.2000,  2.2000],\n",
      "        [ 2.2000,  2.2000,  2.2000,  ...,  2.2000,  2.2000,  2.2000],\n",
      "        [ 2.2000,  2.2000,  2.2000,  ...,  2.2000,  2.2000,  2.2000]])\n"
     ]
    }
   ],
   "source": [
    "words, lengths = item.word\n",
    "words_size = words.size()\n",
    "print('words:', words_size, words)\n",
    "\n",
    "bos_embed = torch.tensor([1.1] * 300)\n",
    "eos_embed = torch.tensor([2.2] * 300)\n",
    "unk_embed = torch.tensor([3.3] * 300)\n",
    "\n",
    "mask = ~((words == WORD.vocab.stoi['<bos>']) | (words == WORD.vocab.stoi['<eos>']) | (words == WORD.vocab.stoi['<unk>']))\n",
    "print('mask:', mask.size(), mask)\n",
    "\n",
    "eos_mask = words == WORD.vocab.stoi['<eos>']\n",
    "unk_mask = words == WORD.vocab.stoi['<unk>']\n",
    "\n",
    "# Embed words\n",
    "embeds = F.embedding(words, WORD.vocab.vectors)\n",
    "\n",
    "print('word embeds:', embeds.size())\n",
    "print('bos embeds:', embeds[bos_mask].size(), embeds[bos_mask])\n",
    "\n",
    "embeds[bos_mask] = bos_embed\n",
    "print('bos embeds:', embeds[bos_mask].size(), embeds[bos_mask])\n",
    "\n",
    "embeds[eos_mask] = eos_embed\n",
    "print('eos embeds:', embeds[eos_mask].size(), embeds[eos_mask])\n",
    "\n",
    "if unk_mask.sum() > 0:\n",
    "    embeds[unk_mask] = unk_embed\n",
    "    print('unk embeds:', embeds[unk_mask].size(), embeds[unk_mask])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "word_embeddings = F.embedding(words, WORD.vocab.vectors)\n",
    "print(word_embeddings.size())\n",
    "packed_words = pack_padded_sequence(word_embeddings, lengths, batch_first=True)\n",
    "print(packed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>: 2\n",
      "<eos>: 3\n",
      "<unk>: 0\n",
      "<pad>: 1\n"
     ]
    }
   ],
   "source": [
    "vocab = WORD.vocab\n",
    "print('<bos>:', vocab.stoi['<bos>'])\n",
    "print('<eos>:', vocab.stoi['<eos>'])\n",
    "print('<unk>:', vocab.stoi['<unk>'])\n",
    "print('<pad>:', vocab.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORD.vocab.extend(glove_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORD.vocab.load_vectors(glove_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExtendedVocab(Vocab):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "class SpecialisedField(Field):\n",
    "    \n",
    "    vocab_cls = ExtendedVocab\n",
    "    \n",
    "    def __init__(**kwargs):\n",
    "        super(SpecialisedField, self).__init__(**kwargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtual-env-3.6.2",
   "language": "python",
   "name": "my-virtual-env-3.6.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
